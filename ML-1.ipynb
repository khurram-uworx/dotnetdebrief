{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßê Numerical Stability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms often involve many mathematical operations, like matrix multiplications, optimizations, and iterative updates. (In this context) Numerical stability refers to how well an algorithm handles floating-point arithmetic errors during calculations. This is crucial because these calculations can lead to small rounding errors that accumulate over time and cause incorrect results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Key Concepts__\n",
    "- _Floating point precision_: Computers represent real numbers in binary (base-2) using a limited number of bits. Commonly, single precision (32 bits) or double precision (64 bits) are used to represent numbers. However, due to limited precision, some real numbers cannot be exactly represented. This can lead to small errors in calculations. For example, the number 1/3 cannot be represented exactly, and rounding errors accumulate during iterative processes.\n",
    "- _Overflow_ occurs when a calculation results in a number too large to be represented within the range of the chosen number format (e.g., 32-bit or 64-bit). For instance, multiplying two large numbers might exceed the maximum number that can be stored\n",
    "- _Underflow_ happens when a calculation results in a number that is too small (close to zero) to be represented within the precision of the system. This often happens in computations involving very small probabilities or when numbers get very close to zero, like in softmax or normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "// Precision Loss Example\n",
    "float sum = 0;\n",
    "for (int i = 0; i < 1000000; i++)\n",
    "    sum += 1e-6f;  // Small increment\n",
    "Console.WriteLine($\"Precision Loss Example: {sum}\");  // Expected to be close to 1, but may be slightly different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÅ NumFlat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/sinshu/numflat\n",
    "\n",
    "The goal of this project is to provide a lightweight package for handling various mathematical and computational tasks, including linear algebra, multivariable analysis, clustering, and signal processing, using only C#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/euclidean-manhattan.png width=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "#r \"nuget: NumFlat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "// https://github.com/sinshu/numflat/blob/main/NumFlatTest/DistanceTests.cs\n",
    "\n",
    "using NumFlat;\n",
    "\n",
    "Vec<double> x = [1, 2, 3]; // the issue of libraries coming up with their implementation of Vector and Tensor\n",
    "Vec<double> y = [1, 3, 7]; // and often will not be beneficiary of any newer runtime optimization/support\n",
    "\n",
    "var euclidean = Distance.Euclidean.GetDistance(x, y); //(x - y).Norm()\n",
    "var manhattan = Distance.Manhattan.GetDistance(x, y); //(x - y).L1Norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "string[] words = { \"clinic\", \"hospital\" };\n",
    "string userInput = \"hopital\";\n",
    "\n",
    "Func<string, int, Vec<double>> vectorize = (word, maxLength) =>\n",
    "{\n",
    "    var paddedWord = word.PadRight(maxLength, '\\0'); // Pad with null characters\n",
    "    return new Vec<double>(paddedWord.Select(c => (double)c).ToArray()); // how character are numbers\n",
    "}; \n",
    "\n",
    "int maxLength = Math.Max(userInput.Length, words.Max(w => w.Length)); // Determining the maximum length\n",
    "var wordVectors = words.Select(word => vectorize(word, maxLength)).ToArray();\n",
    "var userVector = vectorize(userInput, maxLength);\n",
    "\n",
    "// Calculate Euclidean distances\n",
    "var distances = wordVectors.Select(\n",
    "    (vector, index) => (Word: words[index], Distance: Distance.Euclidean.GetDistance(userVector, vector)))\n",
    "    .OrderBy(result => result.Distance);\n",
    "\n",
    "foreach(var distance in distances)\n",
    "    Console.WriteLine($\"Closest match: {distance.Word}, Distance: {distance.Distance}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "static int DamerauLevenshteinDistance(this string s, string t)\n",
    "{\n",
    "    var bounds = new { Height = s.Length + 1, Width = t.Length + 1 };\n",
    "\n",
    "    int[,] matrix = new int[bounds.Height, bounds.Width];\n",
    "\n",
    "    for (int height = 0; height < bounds.Height; height++) { matrix[height, 0] = height; };\n",
    "    for (int width = 0; width < bounds.Width; width++) { matrix[0, width] = width; };\n",
    "\n",
    "    for (int height = 1; height < bounds.Height; height++)\n",
    "    {\n",
    "        for (int width = 1; width < bounds.Width; width++)\n",
    "        {\n",
    "            int cost = (s[height - 1] == t[width - 1]) ? 0 : 1;\n",
    "            int insertion = matrix[height, width - 1] + 1;\n",
    "            int deletion = matrix[height - 1, width] + 1;\n",
    "            int substitution = matrix[height - 1, width - 1] + cost;\n",
    "\n",
    "            int distance = Math.Min(insertion, Math.Min(deletion, substitution));\n",
    "\n",
    "            if (height > 1 && width > 1 && s[height - 1] == t[width - 2] && s[height - 2] == t[width - 1])\n",
    "                distance = Math.Min(distance, matrix[height - 2, width - 2] + cost);\n",
    "\n",
    "            matrix[height, width] = distance;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return matrix[bounds.Height - 1, bounds.Width - 1];\n",
    "}\n",
    "\n",
    "string[] words = { \"clinic\", \"hospital\" };\n",
    "string userInput = \"hopital\";\n",
    "\n",
    "var q = from w in words\n",
    "        select new { word = w, Distance = w.DamerauLevenshteinDistance(userInput) };\n",
    "foreach(var r in q.OrderBy(w => w.Distance))\n",
    "    Console.WriteLine($\"{r.word} {r.Distance}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ChatGPT agrees; you dont need to be Mathematical Guru and can easily figure this out with some basic prompts\n",
    "- https://chatgpt.com/share/678e0c58-c820-800b-aff3-7a2a6ebfd8a2\n",
    "\n",
    "<img src=images/levenshtein-distance.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors can also be created from objects that implement IEnumerable<T>. Since the vector itself is an IEnumerable<T>, it is also possible to call LINQ methods on the vector if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using NumFlat;\n",
    "\n",
    "// Some enumerable.\n",
    "var enumerable = Enumerable.Range(0, 10).Select(i => i / 10.0);\n",
    "\n",
    "// Create a vector from an enumerable.\n",
    "var vector = enumerable.ToVector();\n",
    "\n",
    "// Show the vector.\n",
    "Console.WriteLine(vector);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/whats-vector.png width=900>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Binary Vectors__ contains only binary values (0 or 1)\n",
    "    - often used in feature engineering to represent the presence or absence of a feature \n",
    "- __One-Hot Vectors__ is a binary vector with a single 1 and the rest 0s üëà\n",
    "    - these are Used to to represent categorical data, we encode categorical variables in machine learning models\n",
    "    - House is Urban or Rural; we can encode this categorical data as either [1, 0] or [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Count Vector__ represents the frequency of terms in a document or dataset\n",
    "    - used in text processing (e.g., bag-of-words feature extraction technique)\n",
    "- __Frequency Vectors__ represent the occurrence or frequency of events or features\n",
    "    - Used in text analysis and signal processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Unit Vector__ has magnitude (length) of 1; their values are 0-1, we obtain these numbers normalizing input vector\n",
    "    - these are Used in scenarios where the direction of the vector matters more than its magnitude (cosine similarity)\n",
    "    - [0.6, 0.8] normalized form of [3, 4]\n",
    "- __Probability Vector__ contains values that sum to 1, representing probabilities\n",
    "    - used in classification tasks (eg output of a softmax layer in neural networks)\n",
    "    - [0.1, 0.7, 0.2] (probabilities of three classes)\n",
    "- __Normalized Vectors__ are scaled to have a specific norm (eg L2 norm of 1)\n",
    "    - these are used to standardize vector magnitudes for comparison (eg cosine similarity)\n",
    "    - [0.6, 0.8] (L2-normalized form of [3, 4])\n",
    "        - Euclidean distance is the straight-line distance between two points in space\n",
    "        - sqrt(0.6^2 + 0.8^2) = 1\n",
    "        - 3/5 + 4/5\n",
    "        - sqrt(3^2 + 4^2) = 5\n",
    "    - L2 Normalization aka Euclidean Norm is a technique used to scale vectors so that their L2 norm (Euclidean length) becomes 1. It is commonly used in machine learning, NLP, and deep learning\n",
    "        - __Prevents dominance__: Ensures no single feature dominates due to magnitude\n",
    "        - __Stabilizes optimization__: Helps gradient-based algorithms converge faster\n",
    "        - __Improves generalization__: Reduces sensitivity to feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Embedding Vector__ are dense, low-dimensional representations of high-dimensional data (e.g., words, images, or categories) learned by models\n",
    "    - Often used in natural language processing (e.g., Word2Vec, GloVe) and recommendation systems\n",
    "    - [0.25, -0.1, 0.7] (a 3D embedding of a word)\n",
    "    - __Composite Vectors__ are formed by combining multiple vectors (eg concatenation, averaging) üëà\n",
    "        - these are used in feature engineering and multi-modal learning\n",
    "        - Concatenating word embeddings to form a sentence embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Latent Vectors__ are learned representations in a lower-dimensional space that capture underlying patterns in the data\n",
    "    - these are used in unsupervised learning (eg autoencoders, generative models)\n",
    "    - eg [0.3, -0.5, 0.2] can be a latent representation of an image\n",
    "- __Weight Vectors__ represent the parameters of a model (eg, coefficients in linear regression, weights in a neural network)\n",
    "    - Used in model training and inference\n",
    "    - [0.5, -0.2, 0.3] (weights of a linear model)\n",
    "- __Gradient Vector__ contains the partial derivatives of a function with respect to its parameters\n",
    "    - these are used in optimization algorithms (eg gradient descent) (learning of neural network)\n",
    "- __Eigenvectors__ are special vectors in linear algebra that do not change direction when a linear transformation (like a matrix multiplication) is applied to them. Instead, they only get scaled by a constant factor called the eigenvalue\n",
    "    - these are used in dimensionality reduction (eg Principal Component Analysis / PCA) and spectral clustering\n",
    "    - also used in Graph algorithms, Computer Vision and Quantum mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìê Sparse and Dense Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/vectors-sparse-dense.webp width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sparse Vectors can be generated according to application / business need\n",
    "- Dense Vectors are generated generally using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "record Person(int ID, string Name);\n",
    "record Movie(int ID, string Name);\n",
    "record WatchedMovie(int PersonID, int MovieID);\n",
    "\n",
    "IEnumerable<Person> persons = [\n",
    "    new Person(1, \"Khurram\"),\n",
    "    new Person(2, \"Mohammad\"),\n",
    "    new Person(3, \"Abdullah\")\n",
    "];\n",
    "IEnumerable<Movie> movies = [\n",
    "    new Movie(1, \"The Shawshank Redemption\"),\n",
    "    new Movie(2, \"Top Gun\"),\n",
    "    new Movie(3, \"Man of Steel\")\n",
    "];\n",
    "IEnumerable<WatchedMovie> watchedMovies = [\n",
    "    new WatchedMovie(1, 1), new WatchedMovie(1, 3),\n",
    "    new WatchedMovie(2, 2), new WatchedMovie(2, 3)\n",
    "];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var sparseVectors = persons.Select(person =>\n",
    "    new\n",
    "    {\n",
    "        PersonID = person.ID,\n",
    "        SparseVector = movies.Select(movie =>\n",
    "            watchedMovies.Any(w => w.PersonID == person.ID && w.MovieID == movie.ID) ? 1 : 0\n",
    "        ).ToList()\n",
    "    });\n",
    "\n",
    "foreach (var vector in sparseVectors)\n",
    "    Console.WriteLine($\"PersonID: {vector.PersonID}, Sparse Vector: [{string.Join(\", \", vector.SparseVector)}]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This often involves assigning a real number to each movie watched or skipped, rather than using binary (sparse) values. To create a \"dense vector\" representation\n",
    "- we can map each person's watched movie pattern into a continuous vector space\n",
    "- we can project the sparse vectors into a continuous space\n",
    "- we can use a simple mathematical transformation without needing a full neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 1: One-Hot Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Represent each movie as a unique one-hot vector based on the number of movies\n",
    "- Map the sparse vector (list of watched movie IDs) to a single dense vector by averaging the one-hot vectors of the watched movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var oneHotVectors = movies.Select(movie =>\n",
    "    new \n",
    "    { \n",
    "        MovieID = movie.ID, \n",
    "        OneHotVector = movies.Select(m => m.ID == movie.ID ? 1 : 0).ToList() \n",
    "    }).ToDictionary(x => x.MovieID, x => x.OneHotVector);\n",
    "\n",
    "var denseVectors = persons.Select(person =>\n",
    "{\n",
    "    var watched = watchedMovies.Where(w => w.PersonID == person.ID).Select(w => oneHotVectors[w.MovieID]).ToList();\n",
    "\n",
    "    return new\n",
    "    {\n",
    "        PersonID = person.ID,\n",
    "        DenseVector = watched.Any()\n",
    "            ? watched.Aggregate(new int[movies.Count()],\n",
    "                (acc, vector) => acc.Zip(vector,\n",
    "                    (x, y) => x + y).ToArray()\n",
    "                ).Select(x => (double)x / watched.Count).ToList()\n",
    "            : Enumerable.Repeat(0.0, movies.Count()).ToList()\n",
    "    };\n",
    "});\n",
    "\n",
    "foreach (var vector in denseVectors)\n",
    "    Console.WriteLine($\"PersonID: {vector.PersonID}, Dense Vector: [{string.Join(\", \", vector.DenseVector)}]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What does this mean?__\n",
    "- Each dimension in the dense vector represents a particular movie\n",
    "    - The number of dimensions in the dense vector depends on the problem context and the dataset\n",
    "    - We can reduce the dimensions; if required using techniques like __Principal Component Analysis__\n",
    "- The value in a dimension is the normalized \"contribution\" of that movie to the person‚Äôs watched movies\n",
    "    - Relationships between movies watched by a person\n",
    "    - Magnitude of interaction with each movie\n",
    "    - If someone has watched two out of three movies; say first and third and we are getting [0.5, 0, 0.5]\n",
    "        - This means the person is equally influenced by Movie 1 and Movie 3 in this representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Idea 2: Weighted Movie Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sounds controversial; but mathematically / statistically it is not\n",
    "\n",
    "- Assign a unique weight (e.g., a random number) to each movie, which represents its contribution to the vector\n",
    "- These weights could simulate learned parameters in a neural network\n",
    "- Aggregate Weights per Person: For each person, calculate their dense vector by summing the weights of the movies they have watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "var movieWeights = movies.ToDictionary(\n",
    "    movie => movie.ID,\n",
    "    movie => new Random(movie.ID).NextDouble() // Assign random weights based on Movie ID\n",
    ");\n",
    "\n",
    "var denseVectors = persons.Select(person =>\n",
    "    new\n",
    "    {\n",
    "        PersonID = person.ID,\n",
    "        DenseVector = movies.Select(movie =>\n",
    "            watchedMovies.Any(w => w.PersonID == person.ID && w.MovieID == movie.ID)\n",
    "                ? movieWeights[movie.ID]\n",
    "                : 0.0 // If not watched, weight is 0\n",
    "        ).ToList()\n",
    "    });\n",
    "\n",
    "foreach (var vector in denseVectors)\n",
    "    Console.WriteLine($\"PersonID: {vector.PersonID}, Dense Vector: [{string.Join(\", \", vector.DenseVector)}]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This approach mimics embedding layers in neural networks where categorical features are mapped to a dense vector representation\n",
    "- This is not truly \"learned\" as in a neural network but serves as a conceptual visualization of how dense vectors might look"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse Vectors on surface looks inefficient; but libraries (and vector databases) internally optimizes these vectors using \"sparse representations\"\n",
    "\n",
    "__Sparse Representation__\n",
    "- Dense vector: [0, 0, 3, 0, 0, 7, 0]\n",
    "- Sparse representation: (indices: [2, 5], values: [3, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¢ Floating-point numeric types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers\n",
    "    - Mantissa x Exponent\n",
    "    - 6.63 x 10^-34\n",
    "    - Computers uses base 2; hardware efficiency (Shift Operations)\n",
    "        - 1/3, 2/3, even 1/10\n",
    "            - 1/5 and 1/10 etc; 5 is not a power of 2\n",
    "    - C#' __decimal__ uses base 10\n",
    "- Floating Point Units\n",
    "- Recent Processors can do multiple floating point operations per clock cycle per core\n",
    "    - Intel Recent Processors with AVX512 and FMA can do 32ops on FP64 and 64ops for FP32\n",
    "    - AMD Zen 2/3 can do 16ops on FP64 and 32ops on FP32\n",
    "    - Qualcomm Kyro 4xx and 5xx can do 8ops on FP64 and 16 on FP32\n",
    "    - Samsung Exynos M3 and M4, 3ops on FP64 and 12 on FP32\n",
    "    - https://en.wikipedia.org/wiki/Floating_point_operations_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __FP8__ 1 Byte; no .NET native representation\n",
    "    - Usually 2 decimal places; used in AI/ML (NVIDIA Tensor Cores)\n",
    "- __FP16__ 2 Bytes; System.Half\n",
    "    - No built in type; 3-5 decimal places; introduced in .NET 8\n",
    "- __FP32__ 4 Bytes; System.Single\n",
    "    - __float__ has 6-9 digits precision\n",
    "- __FP64__ 8 Bytes; System.Double\n",
    "    - __double__ has 15-17 digits precision\n",
    "- __FP128__ 16 Bytes; no .NET native representation\n",
    "    - System.Decimal is 16 bytes / 128 bit floating point implementation but its not IEE 754 / FP128 compatible\n",
    "    - __decimal__ has 28-29 digits precision; FP128  has 34; suited for financial, accounting, precise decimal calculations\n",
    "    - FP128/IEE 754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "float f = -5.75f;\n",
    "int intBits = BitConverter.SingleToInt32Bits(f); // IEE754 representation of float / reinterpreting\n",
    "// once we have reinterpretted bits; we can do bit operations and extract sign, mantissa and exponent\n",
    "\n",
    "Console.WriteLine($\"Float: {f}\");\n",
    "Console.WriteLine($\"Binary: {Convert.ToString(intBits, 2).PadLeft(32, '0')}\");\n",
    "Console.WriteLine($\"Hex: 0x{intBits:X8}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Floats and Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Normalizing to [-1, 1]__\n",
    "\n",
    "- Better Distribution for Activation Functions\n",
    "    - Neural networks (especially those with sigmoid or tanh activations) perform better with inputs in this range, preventing vanishing/exploding gradients\n",
    "    - Hyperbolic Tangent function, Tanh outputs naturally range from [-1, 1] making it a good fit\n",
    "- Numerical Stability in Floating-Point Arithmetic\n",
    "    - Floats have higher precision around small values (closer to 0)\n",
    "    - The standard IEEE 754 32-bit float has more precision between -1 and 1 than for very large/small numbers\n",
    "- Prevention of Overflow and Underflow\n",
    "    - Many ML/AI algorithms involve exponentiation, which can easily overflow with large numbers\n",
    "    - Keeping numbers small helps prevent such issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Float Precision in the [-1, 1] Range__\n",
    "\n",
    "Floating-point numbers (IEEE 754) allocate bits as:\n",
    "- 1 bit for sign\n",
    "- 8 bits for exponent\n",
    "- 23 bits for the fraction (mantissa)\n",
    "\n",
    "For floats in [-1, 1]:\n",
    "- The exponent is small, leaving more bits for the fraction (mantissa)\n",
    "- The precision is highest when numbers are close to zero\n",
    "- In this range, a 32-bit float gives approximately 7 decimal places of precision, which is generally enough for ML computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP8 Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FP8 FORMATS FOR DEEP LEARNING\n",
    "- https://arxiv.org/pdf/2209.05433"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "\n",
    "public readonly struct FP8 : IEquatable<FP8>\n",
    "{\n",
    "    readonly byte value;\n",
    "\n",
    "    public FP8(byte value) => this.value = value;\n",
    "\n",
    "    // Convert from float (FP32) to FP8\n",
    "    public static explicit operator FP8(float f)\n",
    "    {\n",
    "        if (float.IsNaN(f)) return new FP8(0x7F); // NaN\n",
    "        if (float.IsPositiveInfinity(f)) return new FP8(0x7C); // +Inf\n",
    "        if (float.IsNegativeInfinity(f)) return new FP8(0xFC); // -Inf\n",
    "        if (f == 0f) return new FP8(0x00); // Zero\n",
    "\n",
    "        int sign = f < 0 ? 1 : 0;\n",
    "        f = Math.Abs(f);\n",
    "\n",
    "        /*\n",
    "         * E5M2 5 bits for exponent and 2 bits for mantissa\n",
    "         * E4M3 will have more precision but lesser range\n",
    "         */\n",
    "\n",
    "        int bias = 15; // E5M2 bias\n",
    "        int maxExp = 31; // Max exponent for E5M2\n",
    "        int mantissaBits = 2; // Mantissa size for E5M2\n",
    "\n",
    "        int floatBits = BitConverter.SingleToInt32Bits(f);\n",
    "        int floatExp = (floatBits >> 23) & 0xFF; // Extracting the exponent (bits 23-30)\n",
    "        int floatMantissa = floatBits & 0x7FFFFF; // Lower 32 bits of Mantissa\n",
    "\n",
    "        if (floatExp == 0) return new FP8((byte)(sign << 7)); // Subnormal / Zero\n",
    "\n",
    "        int exponent = floatExp - 127 + bias;\n",
    "        if (exponent < 0) return new FP8((byte)(sign << 7)); // Underflow to zero\n",
    "        if (exponent > maxExp) return new FP8((byte)((sign << 7) | (maxExp << mantissaBits))); // Overflow to Inf\n",
    "\n",
    "        int mantissa = floatMantissa >> (23 - mantissaBits); // Truncate mantissa\n",
    "        return new FP8((byte)((sign << 7) | (exponent << mantissaBits) | mantissa));\n",
    "    }\n",
    "\n",
    "    // Convert from FP8 to float (FP32)\n",
    "    public static explicit operator float(FP8 fp8)\n",
    "    {\n",
    "        int sign = (fp8.value & 0x80) != 0 ? -1 : 1;\n",
    "        int exponent = (fp8.value >> 2) & 0x1F; // Extract exponent\n",
    "        int mantissa = fp8.value & 0x03; // Extract mantissa\n",
    "        int bias = 15; // E5M2 bias\n",
    "\n",
    "        if (exponent == 0) return sign * (mantissa / 4f); // Subnormal case\n",
    "        if (exponent == 31) return sign * float.PositiveInfinity; // Infinity/NaN\n",
    "\n",
    "        return sign * (1 + mantissa / 4f) * (float)Math.Pow(2, exponent - bias);\n",
    "    }\n",
    "\n",
    "    // Arithmetic operations (via float conversions)\n",
    "    public static FP8 operator +(FP8 a, FP8 b) => (FP8)((float)a + (float)b);\n",
    "    public static FP8 operator -(FP8 a, FP8 b) => (FP8)((float)a - (float)b);\n",
    "    public static FP8 operator *(FP8 a, FP8 b) => (FP8)((float)a * (float)b);\n",
    "    public static FP8 operator /(FP8 a, FP8 b) => (FP8)((float)a / (float)b);\n",
    "\n",
    "    public override bool Equals(object obj) => obj is FP8 other && Equals(other);\n",
    "    public bool Equals(FP8 other) => value == other.value;\n",
    "    public override int GetHashCode() => value.GetHashCode();\n",
    "    public override string ToString() => $\"{(float)this:F6} (0x{value:X2})\";\n",
    "}\n",
    "\n",
    "FP8 a = (FP8)1.5f;\n",
    "FP8 b = (FP8)2.75f;\n",
    "\n",
    "Console.WriteLine($\"a = {a}\");\n",
    "Console.WriteLine($\"b = {b}\");\n",
    "\n",
    "FP8 sum = a + b;\n",
    "Console.WriteLine($\"a + b = {sum} (as float: {(float)sum})\");\n",
    "\n",
    "FP8 difference = a - b;\n",
    "Console.WriteLine($\"a - b = {difference} (as float: {(float)difference})\");\n",
    "\n",
    "FP8 product = a * b;\n",
    "Console.WriteLine($\"a * b = {product} (as float: {(float)product})\");\n",
    "\n",
    "FP8 quotient = a / b;\n",
    "Console.WriteLine($\"a / b = {quotient} (as float: {(float)quotient})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [],
   "source": [
    "using System;\n",
    "using System.Numerics;\n",
    "using System.Globalization;\n",
    "\n",
    "public readonly struct FP8 : INumber<FP8>\n",
    "{\n",
    "    const int Bias = 15, MaxExp = 31, MantissaBits = 2;\n",
    "    readonly byte value;\n",
    "\n",
    "    public FP8(byte value) => this.value = value;\n",
    "\n",
    "    public static explicit operator FP8(float f)\n",
    "        => float.IsNaN(f) ? new FP8(0x7F)\n",
    "         : float.IsPositiveInfinity(f) ? new FP8(0x7C)\n",
    "         : float.IsNegativeInfinity(f) ? new FP8(0xFC)\n",
    "         : f == 0f ? new FP8(0x00)\n",
    "         : CreateFromFloat(f);\n",
    "\n",
    "    private static FP8 CreateFromFloat(float f)\n",
    "    {\n",
    "        int sign = f < 0 ? 1 : 0;\n",
    "        f = Math.Abs(f);\n",
    "        int bits = BitConverter.SingleToInt32Bits(f);\n",
    "        int exp = (bits >> 23) & 0xFF;\n",
    "        int man = bits & 0x7FFFFF;\n",
    "        if (exp == 0) return new FP8((byte)(sign << 7));\n",
    "        int newExp = exp - 127 + Bias;\n",
    "        if (newExp < 0) return new FP8((byte)(sign << 7));\n",
    "        if (newExp > MaxExp) return new FP8((byte)((sign << 7) | (MaxExp << MantissaBits)));\n",
    "        int newMan = man >> (23 - MantissaBits);\n",
    "        return new FP8((byte)((sign << 7) | (newExp << MantissaBits) | newMan));\n",
    "    }\n",
    "\n",
    "    public static explicit operator float(FP8 fp8)\n",
    "    {\n",
    "        int sign = (fp8.value & 0x80) != 0 ? -1 : 1;\n",
    "        int exp = (fp8.value >> MantissaBits) & 0x1F;\n",
    "        int man = fp8.value & 0x03;\n",
    "        if (exp == 0) return sign * (man / 4f);\n",
    "        if (exp == MaxExp) return man == 0 ? sign * float.PositiveInfinity : float.NaN;\n",
    "        return sign * (1 + man / 4f) * (float)Math.Pow(2, exp - Bias);\n",
    "    }\n",
    "\n",
    "    public static FP8 operator +(FP8 a, FP8 b) => (FP8)((float)a + (float)b);\n",
    "    public static FP8 operator -(FP8 a, FP8 b) => (FP8)((float)a - (float)b);\n",
    "    public static FP8 operator *(FP8 a, FP8 b) => (FP8)((float)a * (float)b);\n",
    "    public static FP8 operator /(FP8 a, FP8 b) => (FP8)((float)a / (float)b);\n",
    "    public static FP8 operator %(FP8 a, FP8 b) => (FP8)((float)a % (float)b);\n",
    "\n",
    "    public static FP8 operator +(FP8 value) => value;\n",
    "    public static FP8 operator -(FP8 value) => (FP8)(-((float)value));\n",
    "    public static FP8 operator ++(FP8 value) => value + One;\n",
    "    public static FP8 operator --(FP8 value) => value - One;\n",
    "\n",
    "    public static bool operator ==(FP8 left, FP8 right) => left.Equals(right);\n",
    "    public static bool operator !=(FP8 left, FP8 right) => !left.Equals(right);\n",
    "    public static bool operator <(FP8 left, FP8 right) => ((float)left) < ((float)right);\n",
    "    public static bool operator <=(FP8 left, FP8 right) => ((float)left) <= ((float)right);\n",
    "    public static bool operator >(FP8 left, FP8 right) => ((float)left) > ((float)right);\n",
    "    public static bool operator >=(FP8 left, FP8 right) => ((float)left) >= ((float)right);\n",
    "\n",
    "    public int CompareTo(FP8 other) => ((float)this).CompareTo((float)other);\n",
    "    int IComparable.CompareTo(object? obj)\n",
    "        => obj is FP8 other ? CompareTo(other) : throw new ArgumentException(\"Not a FP8\");\n",
    "\n",
    "    public bool Equals(FP8 other) => value == other.value;\n",
    "    public override bool Equals(object? obj) => obj is FP8 other && Equals(other);\n",
    "    public override int GetHashCode() => value.GetHashCode();\n",
    "\n",
    "    public override string ToString() => ((float)this).ToString(\"G\", CultureInfo.InvariantCulture);\n",
    "    public string ToString(string? format, IFormatProvider? provider) => ((float)this).ToString(format, provider);\n",
    "    public bool TryFormat(Span<char> destination, out int charsWritten, ReadOnlySpan<char> format, IFormatProvider? provider)\n",
    "        => ((float)this).TryFormat(destination, out charsWritten, format, provider);\n",
    "\n",
    "    public static FP8 Parse(string s, IFormatProvider? provider) => (FP8)float.Parse(s, provider);\n",
    "    public static FP8 Parse(string s, NumberStyles style, IFormatProvider? provider) => (FP8)float.Parse(s, style, provider);\n",
    "    public static FP8 Parse(ReadOnlySpan<char> s, IFormatProvider? provider) => (FP8)float.Parse(s.ToString(), provider);\n",
    "    public static FP8 Parse(ReadOnlySpan<char> s, NumberStyles style, IFormatProvider? provider)\n",
    "        => (FP8)float.Parse(s.ToString(), style, provider);\n",
    "\n",
    "    public static bool TryParse(string? s, IFormatProvider? provider, out FP8 result)\n",
    "        => float.TryParse(s, NumberStyles.Float, provider, out float f)\n",
    "            ? (result = (FP8)f, true).Item2\n",
    "            : (result = default, false).Item2;\n",
    "    public static bool TryParse(string? s, NumberStyles style, IFormatProvider? provider, out FP8 result)\n",
    "        => float.TryParse(s, style, provider, out float f)\n",
    "            ? (result = (FP8)f, true).Item2\n",
    "            : (result = default, false).Item2;\n",
    "    public static bool TryParse(ReadOnlySpan<char> s, IFormatProvider? provider, out FP8 result)\n",
    "        => float.TryParse(s.ToString(), NumberStyles.Float, provider, out float f)\n",
    "            ? (result = (FP8)f, true).Item2\n",
    "            : (result = default, false).Item2;\n",
    "    public static bool TryParse(ReadOnlySpan<char> s, NumberStyles style, IFormatProvider? provider, out FP8 result)\n",
    "        => float.TryParse(s.ToString(), style, provider, out float f)\n",
    "            ? (result = (FP8)f, true).Item2\n",
    "            : (result = default, false).Item2;\n",
    "\n",
    "    public static FP8 Abs(FP8 value) => (FP8)Math.Abs((float)value);\n",
    "    public static bool IsCanonical(FP8 value) => true;\n",
    "    public static bool IsComplexNumber(FP8 value) => false;\n",
    "    public static bool IsEvenInteger(FP8 value) => IsInteger(value) && (((long)(float)value) & 1L) == 0;\n",
    "    public static bool IsFinite(FP8 value) => !IsInfinity(value) && !IsNaN(value);\n",
    "    public static bool IsImaginaryNumber(FP8 value) => false;\n",
    "    public static bool IsInfinity(FP8 value)\n",
    "    {\n",
    "        int exp = (value.value >> MantissaBits) & 0x1F;\n",
    "        int man = value.value & 0x03;\n",
    "        return exp == MaxExp && man == 0;\n",
    "    }\n",
    "    public static bool IsInteger(FP8 value) => Math.Floor((float)value) == (float)value;\n",
    "    public static bool IsNaN(FP8 value)\n",
    "    {\n",
    "        int exp = (value.value >> MantissaBits) & 0x1F;\n",
    "        int man = value.value & 0x03;\n",
    "        return exp == MaxExp && man != 0;\n",
    "    }\n",
    "    public static bool IsNegative(FP8 value) => (value.value & 0x80) != 0;\n",
    "    public static bool IsNegativeInfinity(FP8 value) => IsInfinity(value) && IsNegative(value);\n",
    "    public static bool IsNormal(FP8 value) => ((value.value >> MantissaBits) & 0x1F) is not 0 and not MaxExp;\n",
    "    public static bool IsOddInteger(FP8 value) => IsInteger(value) && (((long)(float)value) & 1L) != 0;\n",
    "    public static bool IsPositive(FP8 value) => !IsNegative(value) && !IsZero(value);\n",
    "    public static bool IsPositiveInfinity(FP8 value) => IsInfinity(value) && !IsNegative(value);\n",
    "    public static bool IsRealNumber(FP8 value) => true;\n",
    "    public static bool IsSubnormal(FP8 value) => ((value.value >> MantissaBits) & 0x1F) == 0 && (value.value & 0x7F) != 0;\n",
    "    public static bool IsZero(FP8 value) => (value.value & 0x7F) == 0;\n",
    "\n",
    "    public static FP8 MaxMagnitude(FP8 x, FP8 y) => Math.Abs((float)x) >= Math.Abs((float)y) ? x : y;\n",
    "    public static FP8 MaxMagnitudeNumber(FP8 x, FP8 y) => MaxMagnitude(x, y);\n",
    "    public static FP8 MinMagnitude(FP8 x, FP8 y) => Math.Abs((float)x) <= Math.Abs((float)y) ? x : y;\n",
    "    public static FP8 MinMagnitudeNumber(FP8 x, FP8 y) => MinMagnitude(x, y);\n",
    "\n",
    "    public static int Radix => 2;\n",
    "    public static FP8 AdditiveIdentity => Zero;\n",
    "    public static FP8 MultiplicativeIdentity => One;\n",
    "\n",
    "    public static bool TryConvertFromChecked<TOther>(TOther value, out FP8 result) where TOther : INumberBase<TOther>\n",
    "    {\n",
    "        try { result = (FP8)Convert.ToSingle(value); return true; } catch { result = default; return false; }\n",
    "    }\n",
    "    public static bool TryConvertFromSaturating<TOther>(TOther value, out FP8 result) where TOther : INumberBase<TOther>\n",
    "        => TryConvertFromChecked(value, out result);\n",
    "    public static bool TryConvertFromTruncating<TOther>(TOther value, out FP8 result) where TOther : INumberBase<TOther>\n",
    "        => TryConvertFromChecked(value, out result);\n",
    "    public static bool TryConvertToChecked<TOther>(FP8 value, out TOther result) where TOther : INumberBase<TOther>\n",
    "    {\n",
    "        try { result = (TOther)Convert.ChangeType((float)value, typeof(TOther)); return true; }\n",
    "        catch { result = default!; return false; }\n",
    "    }\n",
    "    public static bool TryConvertToSaturating<TOther>(FP8 value, out TOther result) where TOther : INumberBase<TOther>\n",
    "        => TryConvertToChecked(value, out result);\n",
    "    public static bool TryConvertToTruncating<TOther>(FP8 value, out TOther result) where TOther : INumberBase<TOther>\n",
    "        => TryConvertToChecked(value, out result);\n",
    "\n",
    "    public static FP8 One => (FP8)1f;\n",
    "    public static FP8 Zero => (FP8)0f;\n",
    "    public static FP8 NegativeOne => (FP8)(-1f);\n",
    "}\n",
    "\n",
    "FP8 a = (FP8)1.5f;\n",
    "FP8 b = (FP8)2.75f;\n",
    "\n",
    "Console.WriteLine($\"a = {a}\");\n",
    "Console.WriteLine($\"b = {b}\");\n",
    "\n",
    "FP8 sum = a + b;\n",
    "Console.WriteLine($\"a + b = {sum} (as float: {(float)sum})\");\n",
    "\n",
    "FP8 difference = a - b;\n",
    "Console.WriteLine($\"a - b = {difference} (as float: {(float)difference})\");\n",
    "\n",
    "FP8 product = a * b;\n",
    "Console.WriteLine($\"a * b = {product} (as float: {(float)product})\");\n",
    "\n",
    "FP8 quotient = a / b;\n",
    "Console.WriteLine($\"a / b = {quotient} (as float: {(float)quotient})\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FP1.58b Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "polyglot_notebook": {
     "kernelName": "csharp"
    },
    "vscode": {
     "languageId": "polyglot-notebook"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:\n",
      "0, -1, 1, 0\n",
      "-1, 1, 0, -1\n"
     ]
    }
   ],
   "source": [
    "using System;\n",
    "using System.Runtime.Intrinsics;\n",
    "using System.Runtime.Intrinsics.X86;\n",
    "\n",
    "public static class TernaryLutAvx\n",
    "{\n",
    "    static readonly byte[] AddLut = new byte[9] {\n",
    "        0, 0, 1,  // -1 + {-1, 0, 1}\n",
    "        0, 1, 2,  //  0 + {-1, 0, 1}\n",
    "        1, 2, 2   //  1 + {-1, 0, 1}\n",
    "    };\n",
    "\n",
    "    public static byte AddPackedByte(byte a, byte b)\n",
    "    {\n",
    "        byte result = 0;\n",
    "\n",
    "        for (int i = 0; i < 4; i++)\n",
    "        {\n",
    "            int va = (a >> (i * 2)) & 0b11;\n",
    "            int vb = (b >> (i * 2)) & 0b11;\n",
    "\n",
    "            int idx = va * 3 + vb;\n",
    "            byte sum = AddLut[idx];\n",
    "\n",
    "            result |= (byte)(sum << (i * 2));\n",
    "        }\n",
    "\n",
    "        return result;\n",
    "    }\n",
    "\n",
    "    public static void AddPackedTernary(byte[] a, byte[] b, byte[] result)\n",
    "    {\n",
    "        if (!Avx2.IsSupported) throw new PlatformNotSupportedException(\"AVX2 not supported\");\n",
    "        int len = a.Length;\n",
    "        int i = 0;\n",
    "\n",
    "        unsafe\n",
    "        {\n",
    "            fixed (byte* pa = a, pb = b, pr = result)\n",
    "            {\n",
    "                for (; i <= len - 32; i += 32)\n",
    "                {\n",
    "                    var va = Avx.LoadVector256(pa + i);\n",
    "                    var vb = Avx.LoadVector256(pb + i);\n",
    "\n",
    "                    for (int j = 0; j < 32; j++)\n",
    "                        pr[i + j] = AddPackedByte(pa[i + j], pb[i + j]);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for (; i < len; i++)\n",
    "            result[i] = AddPackedByte(a[i], b[i]);\n",
    "    }\n",
    "\n",
    "    public static int[] Unpack(byte packed)\n",
    "    {\n",
    "        int[] result = new int[4];\n",
    "        for (int i = 0; i < 4; i++)\n",
    "        {\n",
    "            int val = (packed >> (i * 2)) & 0b11;\n",
    "            result[i] = val switch\n",
    "            {\n",
    "                0 => -1,\n",
    "                1 => 0,\n",
    "                2 => 1,\n",
    "                _ => 0 // reserved\n",
    "            };\n",
    "        }\n",
    "        return result;\n",
    "    }\n",
    "\n",
    "    public static byte Pack(params int[] values)\n",
    "    {\n",
    "        byte packed = 0;\n",
    "        for (int i = 0; i < values.Length && i < 4; i++)\n",
    "        {\n",
    "            byte val = values[i] switch\n",
    "            {\n",
    "                -1 => 0,\n",
    "                0 => 1,\n",
    "                1 => 2,\n",
    "                _ => throw new ArgumentException()\n",
    "            };\n",
    "            packed |= (byte)(val << (i * 2));\n",
    "        }\n",
    "        return packed;\n",
    "    }\n",
    "}\n",
    "\n",
    "var a = new byte[]\n",
    "{\n",
    "    TernaryLutAvx.Pack(-1, 0, 1, -1),\n",
    "    TernaryLutAvx.Pack(0, 1, -1, 0)\n",
    "};\n",
    "var b = new byte[]\n",
    "{\n",
    "    TernaryLutAvx.Pack(1, -1, 0, 1),\n",
    "    TernaryLutAvx.Pack(-1, 0, 1, -1)\n",
    "};\n",
    "\n",
    "var result = new byte[a.Length];\n",
    "// Perform ternary addition\n",
    "TernaryLutAvx.AddPackedTernary(a, b, result);\n",
    "\n",
    "// Display unpacked result\n",
    "Console.WriteLine(\"Result:\");\n",
    "foreach (var packed in result)\n",
    "{\n",
    "    var unpacked = TernaryLutAvx.Unpack(packed);\n",
    "    Console.WriteLine(string.Join(\", \", unpacked));\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://qdrant.tech/articles/sparse-vectors\n",
    "- https://www.pinecone.io/learn/series/nlp/dense-vector-embeddings-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/PowerShell/PowerShell\n",
    "- https://github.com/microsoft/terminal\n",
    "- https://dotnet.microsoft.com\n",
    "    - https://github.com/dotnet\n",
    "- https://code.visualstudio.com/\n",
    "    - https://github.com/microsoft/vscode\n",
    "    - https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode\n",
    "    - https://github.com/dotnet/interactive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     },
     {
      "aliases": [],
      "languageName": "python",
      "name": "pythonkernel"
     },
     {
      "aliases": [],
      "languageName": "R",
      "name": "Rkernel"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
